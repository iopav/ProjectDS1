{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os   \n",
    "\n",
    "# from data import OralCancerDataset, RotationTransform\n",
    "# from utils import save_checkpoint, load_checkpoint, latest_checkpoint_path\n",
    "# from process import train_epoch, validate_epoch, infer\n",
    "# from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 224\n",
    "num_epochs = 1000\n",
    "learning_rate = 2e-05 #3e-5 10-  5e-5 reg\n",
    "#min_learning_rate = 5e-06\n",
    "betas = (0.9, 0.999)\n",
    "num_workers = 4\n",
    "weight_decay = 0 #2e-05\n",
    "pin_memory = True \n",
    "drop_path_rate = 0.3\n",
    "gamma = 0.7\n",
    "\n",
    "save_freq = 10\n",
    "\n",
    "path_to_csv = 'cancer-classification-challenge-2024/train.csv'\n",
    "path_to_train_images = 'cancer-classification-challenge-2024/train'\n",
    "path_to_test_images = 'cancer-classification-challenge-2024/test'\n",
    "model_dir = 'checkpoints/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    RotationTransform(angles=[0, -90, 90, 180]),\n",
    "    transforms.Resize((128, 128)), \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OralCancerDataset(path_to_csv=path_to_csv, path_to_image=path_to_train_images, train=1, transform=transform, val_split=0.5)\n",
    "val_dataset = OralCancerDataset(path_to_csv=path_to_csv, path_to_image=path_to_train_images, train=0, transform=transform_val, val_split=0.5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_convnext_tiny_AvgPolling(drop_path_rate=drop_path_rate).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,start_factor=1,end_factor=0.1,total_iters=20)\n",
    "writer = SummaryWriter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0/220 Train loss: 0.493014, Val loss: 0.403734, Train acc: 0.753466, Val acc: 80.967039, Val AUC: 0.874111, LR: 0.000019\n",
      "Epoch:   1/220 Train loss: 0.375581, Val loss: 0.350352, Train acc: 0.825871, Val acc: 83.830019, Val AUC: 0.908684, LR: 0.000018\n",
      "Epoch:   2/220 Train loss: 0.320417, Val loss: 0.305062, Train acc: 0.855646, Val acc: 86.120948, Val AUC: 0.927207, LR: 0.000017\n",
      "Epoch:   3/220 Train loss: 0.291243, Val loss: 0.284201, Train acc: 0.870804, Val acc: 87.324980, Val AUC: 0.937182, LR: 0.000016\n",
      "Epoch:   4/220 Train loss: 0.263542, Val loss: 0.271068, Train acc: 0.883870, Val acc: 87.771724, Val AUC: 0.946319, LR: 0.000015\n",
      "Epoch:   5/220 Train loss: 0.246503, Val loss: 0.260856, Train acc: 0.892129, Val acc: 88.311087, Val AUC: 0.950406, LR: 0.000015\n",
      "Epoch:   6/220 Train loss: 0.231341, Val loss: 0.251201, Train acc: 0.900369, Val acc: 88.926723, Val AUC: 0.954497, LR: 0.000014\n",
      "Epoch:   7/220 Train loss: 0.215300, Val loss: 0.239858, Train acc: 0.908147, Val acc: 89.555979, Val AUC: 0.957469, LR: 0.000013\n",
      "Epoch:   8/220 Train loss: 0.201278, Val loss: 0.234760, Train acc: 0.915469, Val acc: 89.953691, Val AUC: 0.960053, LR: 0.000012\n",
      "Saving model and optimizer state at epoch 8 to checkpoints/checkpoint_8.pth\n",
      "Model saved at epoch 8, val loss: 0.23475979404841982\n",
      "Epoch:   9/220 Train loss: 0.190967, Val loss: 0.234223, Train acc: 0.919156, Val acc: 90.054481, Val AUC: 0.962003, LR: 0.000011\n",
      "Saving model and optimizer state at epoch 9 to checkpoints/checkpoint_9.pth\n",
      "Model saved at epoch 9, val loss: 0.2342230639443165\n",
      "Epoch:  10/220 Train loss: 0.182151, Val loss: 0.225961, Train acc: 0.924953, Val acc: 90.452193, Val AUC: 0.964578, LR: 0.000010\n",
      "Saving model and optimizer state at epoch 10 to checkpoints/checkpoint_10.pth\n",
      "Model saved at epoch 10, val loss: 0.22596110885099666\n",
      "Epoch:  11/220 Train loss: 0.170191, Val loss: 0.225339, Train acc: 0.931003, Val acc: 90.689186, Val AUC: 0.966062, LR: 0.000009\n",
      "Saving model and optimizer state at epoch 11 to checkpoints/checkpoint_11.pth\n",
      "Model saved at epoch 11, val loss: 0.22533888569692287\n",
      "Epoch:  12/220 Train loss: 0.161020, Val loss: 0.217384, Train acc: 0.932855, Val acc: 90.877145, Val AUC: 0.967272, LR: 0.000008\n",
      "Saving model and optimizer state at epoch 12 to checkpoints/checkpoint_12.pth\n",
      "Model saved at epoch 12, val loss: 0.21738426514515063\n",
      "Epoch:  13/220 Train loss: 0.154293, Val loss: 0.217510, Train acc: 0.937742, Val acc: 91.081449, Val AUC: 0.968835, LR: 0.000007\n",
      "Saving model and optimizer state at epoch 13 to checkpoints/checkpoint_13.pth\n",
      "Model saved at epoch 13, val auc: 0.968835297743092\n",
      "Epoch:  14/220 Train loss: 0.147642, Val loss: 0.215394, Train acc: 0.940333, Val acc: 91.043312, Val AUC: 0.968808, LR: 0.000006\n",
      "Saving model and optimizer state at epoch 14 to checkpoints/checkpoint_14.pth\n",
      "Model saved at epoch 14, val loss: 0.21539422760649427\n",
      "Epoch:  15/220 Train loss: 0.139763, Val loss: 0.215856, Train acc: 0.944835, Val acc: 91.247616, Val AUC: 0.970089, LR: 0.000006\n",
      "Saving model and optimizer state at epoch 15 to checkpoints/checkpoint_15.pth\n",
      "Model saved at epoch 15, val auc: 0.970088969301763\n",
      "Epoch:  16/220 Train loss: 0.136372, Val loss: 0.213316, Train acc: 0.946119, Val acc: 91.332062, Val AUC: 0.970115, LR: 0.000005\n",
      "Saving model and optimizer state at epoch 16 to checkpoints/checkpoint_16.pth\n",
      "Model saved at epoch 16, val loss: 0.21331626612965654\n",
      "Epoch:  17/220 Train loss: 0.131784, Val loss: 0.210846, Train acc: 0.947460, Val acc: 91.470989, Val AUC: 0.970655, LR: 0.000004\n",
      "Saving model and optimizer state at epoch 17 to checkpoints/checkpoint_17.pth\n",
      "Model saved at epoch 17, val loss: 0.21084570403142674\n",
      "Epoch:  18/220 Train loss: 0.128099, Val loss: 0.211791, Train acc: 0.949179, Val acc: 91.549986, Val AUC: 0.971732, LR: 0.000003\n",
      "Saving model and optimizer state at epoch 18 to checkpoints/checkpoint_18.pth\n",
      "Model saved at epoch 18, val auc: 0.9717315145399369\n",
      "Epoch:  19/220 Train loss: 0.121973, Val loss: 0.210266, Train acc: 0.952749, Val acc: 91.601743, Val AUC: 0.971983, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 19 to checkpoints/checkpoint_19.pth\n",
      "Model saved at epoch 19, val loss: 0.21026606594280497\n",
      "Epoch:  20/220 Train loss: 0.121987, Val loss: 0.210265, Train acc: 0.953329, Val acc: 91.626260, Val AUC: 0.972316, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 20 to checkpoints/checkpoint_20.pth\n",
      "Model saved at epoch 20, val loss: 0.21026537022212657\n",
      "Epoch:  21/220 Train loss: 0.119204, Val loss: 0.211341, Train acc: 0.952866, Val acc: 91.577227, Val AUC: 0.972467, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 21 to checkpoints/checkpoint_21.pth\n",
      "Model saved at epoch 21, val auc: 0.9724665531347353\n",
      "Epoch:  22/220 Train loss: 0.119451, Val loss: 0.212289, Train acc: 0.953508, Val acc: 91.599019, Val AUC: 0.972336, LR: 0.000002\n",
      "Epoch:  23/220 Train loss: 0.116675, Val loss: 0.210674, Train acc: 0.955261, Val acc: 91.713430, Val AUC: 0.972478, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 23 to checkpoints/checkpoint_23.pth\n",
      "Model saved at epoch 23, val auc: 0.9724775326411257\n",
      "Epoch:  24/220 Train loss: 0.115531, Val loss: 0.211049, Train acc: 0.954686, Val acc: 91.653500, Val AUC: 0.972325, LR: 0.000002\n",
      "Epoch:  25/220 Train loss: 0.115585, Val loss: 0.210339, Train acc: 0.954153, Val acc: 91.716154, Val AUC: 0.972850, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 25 to checkpoints/checkpoint_25.pth\n",
      "Model saved at epoch 25, val auc: 0.9728501888546807\n",
      "Epoch:  26/220 Train loss: 0.112189, Val loss: 0.210885, Train acc: 0.956529, Val acc: 91.737946, Val AUC: 0.972800, LR: 0.000002\n",
      "Epoch:  27/220 Train loss: 0.112644, Val loss: 0.209445, Train acc: 0.955612, Val acc: 91.792427, Val AUC: 0.973174, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 27 to checkpoints/checkpoint_27.pth\n",
      "Model saved at epoch 27, val loss: 0.20944545545229099\n",
      "Epoch:  28/220 Train loss: 0.111619, Val loss: 0.212482, Train acc: 0.957905, Val acc: 91.735222, Val AUC: 0.973281, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 28 to checkpoints/checkpoint_28.pth\n",
      "Model saved at epoch 28, val auc: 0.9732813980896982\n",
      "Epoch:  29/220 Train loss: 0.110865, Val loss: 0.210128, Train acc: 0.957466, Val acc: 91.751566, Val AUC: 0.973436, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 29 to checkpoints/checkpoint_29.pth\n",
      "Model saved at epoch 29, val auc: 0.9734357663315407\n",
      "Epoch:  30/220 Train loss: 0.110074, Val loss: 0.211870, Train acc: 0.957209, Val acc: 91.767911, Val AUC: 0.973626, LR: 0.000002\n",
      "Saving model and optimizer state at epoch 30 to checkpoints/checkpoint_30.pth\n",
      "Model saved at epoch 30, val auc: 0.9736255323585691\n",
      "Epoch:  31/220 Train loss: 0.107745, Val loss: 0.210230, Train acc: 0.959445, Val acc: 91.876873, Val AUC: 0.973490, LR: 0.000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m     start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 15\u001b[0m     train_loss, trian_acc, lr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     val_loss, val_acc, val_auc \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, loss_fn, epoch, writer, device)\n\u001b[0;32m     18\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss, epoch)\n",
      "File \u001b[1;32me:\\Assignment5\\process.py:21\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, scheduler, loss_fn, train_loader, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m     l\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 21\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m running_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     23\u001b[0m running_acc  \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_min_val_loss = 1000\n",
    "current_max_val_auc = 0\n",
    "\n",
    "try:\n",
    "    _, _, _, start_epoch = load_checkpoint(latest_checkpoint_path(model_dir, \"checkpoint_*.pth\"), model, optimizer)\n",
    "    current_min_val_loss,_ , current_max_val_auc = validate_epoch(model, val_loader, loss_fn, start_epoch, writer, device)\n",
    "    start_epoch += 1\n",
    "    print(f\"Starting from epoch {start_epoch}, min loss: {current_min_val_loss}, max auc: {current_max_val_auc}\")\n",
    "except:\n",
    "    start_epoch = 0\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    \n",
    "    train_loss, trian_acc, lr = train_epoch(model, optimizer, scheduler, loss_fn, train_loader, device)\n",
    "    val_loss, val_acc, val_auc = validate_epoch(model, val_loader, loss_fn, epoch, writer, device)\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"HyperParameters/LearningRate\", lr , epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", trian_acc, epoch)\n",
    "    writer.add_scalar(\"AUC/val\", val_auc, epoch)\n",
    "    print(f\"Epoch: {epoch:3d}/{num_epochs:3d} Train loss: {train_loss:6f}, Val loss: {val_loss:6f}, Train acc: {trian_acc:6f}, Val acc: {val_acc:6f}, Val AUC: {val_auc:6f}, LR: {lr:6f}\")\n",
    "\n",
    "    if epoch > 7 and val_loss < current_min_val_loss:\n",
    "        current_min_val_loss = val_loss\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, model_dir, max_checkpoints = 10)\n",
    "        print(f\"Model saved at epoch {epoch}, val loss: {val_loss}\")\n",
    "    elif epoch > 7 and val_auc > current_max_val_auc:\n",
    "        current_max_val_auc = val_auc\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, model_dir, max_checkpoints = 10)\n",
    "        print(f\"Model saved at epoch {epoch}, val auc: {val_auc}\")\n",
    "    elif epoch > 1 and epoch%save_freq == 0:\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, model_dir, max_checkpoints = 10)\n",
    "        print(f\"Model saved at epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/checkpoint_12.pth' (epoch 12)\n",
      "Processed 0/66530 images\n",
      "Processed 100/66530 images\n",
      "Processed 200/66530 images\n",
      "Processed 300/66530 images\n",
      "Processed 400/66530 images\n",
      "Processed 500/66530 images\n",
      "Processed 600/66530 images\n",
      "Processed 700/66530 images\n",
      "Processed 800/66530 images\n",
      "Processed 900/66530 images\n",
      "Processed 1000/66530 images\n",
      "Processed 1100/66530 images\n",
      "Processed 1200/66530 images\n",
      "Processed 1300/66530 images\n",
      "Processed 1400/66530 images\n",
      "Processed 1500/66530 images\n",
      "Processed 1600/66530 images\n",
      "Processed 1700/66530 images\n",
      "Processed 1800/66530 images\n",
      "Processed 1900/66530 images\n",
      "Processed 2000/66530 images\n",
      "Processed 2100/66530 images\n",
      "Processed 2200/66530 images\n",
      "Processed 2300/66530 images\n",
      "Processed 2400/66530 images\n",
      "Processed 2500/66530 images\n",
      "Processed 2600/66530 images\n",
      "Processed 2700/66530 images\n",
      "Processed 2800/66530 images\n",
      "Processed 2900/66530 images\n",
      "Processed 3000/66530 images\n",
      "Processed 3100/66530 images\n",
      "Processed 3200/66530 images\n",
      "Processed 3300/66530 images\n",
      "Processed 3400/66530 images\n",
      "Processed 3500/66530 images\n",
      "Processed 3600/66530 images\n",
      "Processed 3700/66530 images\n",
      "Processed 3800/66530 images\n",
      "Processed 3900/66530 images\n",
      "Processed 4000/66530 images\n",
      "Processed 4100/66530 images\n",
      "Processed 4200/66530 images\n",
      "Processed 4300/66530 images\n",
      "Processed 4400/66530 images\n",
      "Processed 4500/66530 images\n",
      "Processed 4600/66530 images\n",
      "Processed 4700/66530 images\n",
      "Processed 4800/66530 images\n",
      "Processed 4900/66530 images\n",
      "Processed 5000/66530 images\n",
      "Processed 5100/66530 images\n",
      "Processed 5200/66530 images\n",
      "Processed 5300/66530 images\n",
      "Processed 5400/66530 images\n",
      "Processed 5500/66530 images\n",
      "Processed 5600/66530 images\n",
      "Processed 5700/66530 images\n",
      "Processed 5800/66530 images\n",
      "Processed 5900/66530 images\n",
      "Processed 6000/66530 images\n",
      "Processed 6100/66530 images\n",
      "Processed 6200/66530 images\n",
      "Processed 6300/66530 images\n",
      "Processed 6400/66530 images\n",
      "Processed 6500/66530 images\n",
      "Processed 6600/66530 images\n",
      "Processed 6700/66530 images\n",
      "Processed 6800/66530 images\n",
      "Processed 6900/66530 images\n",
      "Processed 7000/66530 images\n",
      "Processed 7100/66530 images\n",
      "Processed 7200/66530 images\n",
      "Processed 7300/66530 images\n",
      "Processed 7400/66530 images\n",
      "Processed 7500/66530 images\n",
      "Processed 7600/66530 images\n",
      "Processed 7700/66530 images\n",
      "Processed 7800/66530 images\n",
      "Processed 7900/66530 images\n",
      "Processed 8000/66530 images\n",
      "Processed 8100/66530 images\n",
      "Processed 8200/66530 images\n",
      "Processed 8300/66530 images\n",
      "Processed 8400/66530 images\n",
      "Processed 8500/66530 images\n",
      "Processed 8600/66530 images\n",
      "Processed 8700/66530 images\n",
      "Processed 8800/66530 images\n",
      "Processed 8900/66530 images\n",
      "Processed 9000/66530 images\n",
      "Processed 9100/66530 images\n",
      "Processed 9200/66530 images\n",
      "Processed 9300/66530 images\n",
      "Processed 9400/66530 images\n",
      "Processed 9500/66530 images\n",
      "Processed 9600/66530 images\n",
      "Processed 9700/66530 images\n",
      "Processed 9800/66530 images\n",
      "Processed 9900/66530 images\n",
      "Processed 10000/66530 images\n",
      "Processed 10100/66530 images\n",
      "Processed 10200/66530 images\n",
      "Processed 10300/66530 images\n",
      "Processed 10400/66530 images\n",
      "Processed 10500/66530 images\n",
      "Processed 10600/66530 images\n",
      "Processed 10700/66530 images\n",
      "Processed 10800/66530 images\n",
      "Processed 10900/66530 images\n",
      "Processed 11000/66530 images\n",
      "Processed 11100/66530 images\n",
      "Processed 11200/66530 images\n",
      "Processed 11300/66530 images\n",
      "Processed 11400/66530 images\n",
      "Processed 11500/66530 images\n",
      "Processed 11600/66530 images\n",
      "Processed 11700/66530 images\n",
      "Processed 11800/66530 images\n",
      "Processed 11900/66530 images\n",
      "Processed 12000/66530 images\n",
      "Processed 12100/66530 images\n",
      "Processed 12200/66530 images\n",
      "Processed 12300/66530 images\n",
      "Processed 12400/66530 images\n",
      "Processed 12500/66530 images\n",
      "Processed 12600/66530 images\n",
      "Processed 12700/66530 images\n",
      "Processed 12800/66530 images\n",
      "Processed 12900/66530 images\n",
      "Processed 13000/66530 images\n",
      "Processed 13100/66530 images\n",
      "Processed 13200/66530 images\n",
      "Processed 13300/66530 images\n",
      "Processed 13400/66530 images\n",
      "Processed 13500/66530 images\n",
      "Processed 13600/66530 images\n",
      "Processed 13700/66530 images\n",
      "Processed 13800/66530 images\n",
      "Processed 13900/66530 images\n",
      "Processed 14000/66530 images\n",
      "Processed 14100/66530 images\n",
      "Processed 14200/66530 images\n",
      "Processed 14300/66530 images\n",
      "Processed 14400/66530 images\n",
      "Processed 14500/66530 images\n",
      "Processed 14600/66530 images\n",
      "Processed 14700/66530 images\n",
      "Processed 14800/66530 images\n",
      "Processed 14900/66530 images\n",
      "Processed 15000/66530 images\n",
      "Processed 15100/66530 images\n",
      "Processed 15200/66530 images\n",
      "Processed 15300/66530 images\n",
      "Processed 15400/66530 images\n",
      "Processed 15500/66530 images\n",
      "Processed 15600/66530 images\n",
      "Processed 15700/66530 images\n",
      "Processed 15800/66530 images\n",
      "Processed 15900/66530 images\n",
      "Processed 16000/66530 images\n",
      "Processed 16100/66530 images\n",
      "Processed 16200/66530 images\n",
      "Processed 16300/66530 images\n",
      "Processed 16400/66530 images\n",
      "Processed 16500/66530 images\n",
      "Processed 16600/66530 images\n",
      "Processed 16700/66530 images\n",
      "Processed 16800/66530 images\n",
      "Processed 16900/66530 images\n",
      "Processed 17000/66530 images\n",
      "Processed 17100/66530 images\n",
      "Processed 17200/66530 images\n",
      "Processed 17300/66530 images\n",
      "Processed 17400/66530 images\n",
      "Processed 17500/66530 images\n",
      "Processed 17600/66530 images\n",
      "Processed 17700/66530 images\n",
      "Processed 17800/66530 images\n",
      "Processed 17900/66530 images\n",
      "Processed 18000/66530 images\n",
      "Processed 18100/66530 images\n",
      "Processed 18200/66530 images\n",
      "Processed 18300/66530 images\n",
      "Processed 18400/66530 images\n",
      "Processed 18500/66530 images\n",
      "Processed 18600/66530 images\n",
      "Processed 18700/66530 images\n",
      "Processed 18800/66530 images\n",
      "Processed 18900/66530 images\n",
      "Processed 19000/66530 images\n",
      "Processed 19100/66530 images\n",
      "Processed 19200/66530 images\n",
      "Processed 19300/66530 images\n",
      "Processed 19400/66530 images\n",
      "Processed 19500/66530 images\n",
      "Processed 19600/66530 images\n",
      "Processed 19700/66530 images\n",
      "Processed 19800/66530 images\n",
      "Processed 19900/66530 images\n",
      "Processed 20000/66530 images\n",
      "Processed 20100/66530 images\n",
      "Processed 20200/66530 images\n",
      "Processed 20300/66530 images\n",
      "Processed 20400/66530 images\n",
      "Processed 20500/66530 images\n",
      "Processed 20600/66530 images\n",
      "Processed 20700/66530 images\n",
      "Processed 20800/66530 images\n",
      "Processed 20900/66530 images\n",
      "Processed 21000/66530 images\n",
      "Processed 21100/66530 images\n",
      "Processed 21200/66530 images\n",
      "Processed 21300/66530 images\n",
      "Processed 21400/66530 images\n",
      "Processed 21500/66530 images\n",
      "Processed 21600/66530 images\n",
      "Processed 21700/66530 images\n",
      "Processed 21800/66530 images\n",
      "Processed 21900/66530 images\n",
      "Processed 22000/66530 images\n",
      "Processed 22100/66530 images\n",
      "Processed 22200/66530 images\n",
      "Processed 22300/66530 images\n",
      "Processed 22400/66530 images\n",
      "Processed 22500/66530 images\n",
      "Processed 22600/66530 images\n",
      "Processed 22700/66530 images\n",
      "Processed 22800/66530 images\n",
      "Processed 22900/66530 images\n",
      "Processed 23000/66530 images\n",
      "Processed 23100/66530 images\n",
      "Processed 23200/66530 images\n",
      "Processed 23300/66530 images\n",
      "Processed 23400/66530 images\n",
      "Processed 23500/66530 images\n",
      "Processed 23600/66530 images\n",
      "Processed 23700/66530 images\n",
      "Processed 23800/66530 images\n",
      "Processed 23900/66530 images\n",
      "Processed 24000/66530 images\n",
      "Processed 24100/66530 images\n",
      "Processed 24200/66530 images\n",
      "Processed 24300/66530 images\n",
      "Processed 24400/66530 images\n",
      "Processed 24500/66530 images\n",
      "Processed 24600/66530 images\n",
      "Processed 24700/66530 images\n",
      "Processed 24800/66530 images\n",
      "Processed 24900/66530 images\n",
      "Processed 25000/66530 images\n",
      "Processed 25100/66530 images\n",
      "Processed 25200/66530 images\n",
      "Processed 25300/66530 images\n",
      "Processed 25400/66530 images\n",
      "Processed 25500/66530 images\n",
      "Processed 25600/66530 images\n",
      "Processed 25700/66530 images\n",
      "Processed 25800/66530 images\n",
      "Processed 25900/66530 images\n",
      "Processed 26000/66530 images\n",
      "Processed 26100/66530 images\n",
      "Processed 26200/66530 images\n",
      "Processed 26300/66530 images\n",
      "Processed 26400/66530 images\n",
      "Processed 26500/66530 images\n",
      "Processed 26600/66530 images\n",
      "Processed 26700/66530 images\n",
      "Processed 26800/66530 images\n",
      "Processed 26900/66530 images\n",
      "Processed 27000/66530 images\n",
      "Processed 27100/66530 images\n",
      "Processed 27200/66530 images\n",
      "Processed 27300/66530 images\n",
      "Processed 27400/66530 images\n",
      "Processed 27500/66530 images\n",
      "Processed 27600/66530 images\n",
      "Processed 27700/66530 images\n",
      "Processed 27800/66530 images\n",
      "Processed 27900/66530 images\n",
      "Processed 28000/66530 images\n",
      "Processed 28100/66530 images\n",
      "Processed 28200/66530 images\n",
      "Processed 28300/66530 images\n",
      "Processed 28400/66530 images\n",
      "Processed 28500/66530 images\n",
      "Processed 28600/66530 images\n",
      "Processed 28700/66530 images\n",
      "Processed 28800/66530 images\n",
      "Processed 28900/66530 images\n",
      "Processed 29000/66530 images\n",
      "Processed 29100/66530 images\n",
      "Processed 29200/66530 images\n",
      "Processed 29300/66530 images\n",
      "Processed 29400/66530 images\n",
      "Processed 29500/66530 images\n",
      "Processed 29600/66530 images\n",
      "Processed 29700/66530 images\n",
      "Processed 29800/66530 images\n",
      "Processed 29900/66530 images\n",
      "Processed 30000/66530 images\n",
      "Processed 30100/66530 images\n",
      "Processed 30200/66530 images\n",
      "Processed 30300/66530 images\n",
      "Processed 30400/66530 images\n",
      "Processed 30500/66530 images\n",
      "Processed 30600/66530 images\n",
      "Processed 30700/66530 images\n",
      "Processed 30800/66530 images\n",
      "Processed 30900/66530 images\n",
      "Processed 31000/66530 images\n",
      "Processed 31100/66530 images\n",
      "Processed 31200/66530 images\n",
      "Processed 31300/66530 images\n",
      "Processed 31400/66530 images\n",
      "Processed 31500/66530 images\n",
      "Processed 31600/66530 images\n",
      "Processed 31700/66530 images\n",
      "Processed 31800/66530 images\n",
      "Processed 31900/66530 images\n",
      "Processed 32000/66530 images\n",
      "Processed 32100/66530 images\n",
      "Processed 32200/66530 images\n",
      "Processed 32300/66530 images\n",
      "Processed 32400/66530 images\n",
      "Processed 32500/66530 images\n",
      "Processed 32600/66530 images\n",
      "Processed 32700/66530 images\n",
      "Processed 32800/66530 images\n",
      "Processed 32900/66530 images\n",
      "Processed 33000/66530 images\n",
      "Processed 33100/66530 images\n",
      "Processed 33200/66530 images\n",
      "Processed 33300/66530 images\n",
      "Processed 33400/66530 images\n",
      "Processed 33500/66530 images\n",
      "Processed 33600/66530 images\n",
      "Processed 33700/66530 images\n",
      "Processed 33800/66530 images\n",
      "Processed 33900/66530 images\n",
      "Processed 34000/66530 images\n",
      "Processed 34100/66530 images\n",
      "Processed 34200/66530 images\n",
      "Processed 34300/66530 images\n",
      "Processed 34400/66530 images\n",
      "Processed 34500/66530 images\n",
      "Processed 34600/66530 images\n",
      "Processed 34700/66530 images\n",
      "Processed 34800/66530 images\n",
      "Processed 34900/66530 images\n",
      "Processed 35000/66530 images\n",
      "Processed 35100/66530 images\n",
      "Processed 35200/66530 images\n",
      "Processed 35300/66530 images\n",
      "Processed 35400/66530 images\n",
      "Processed 35500/66530 images\n",
      "Processed 35600/66530 images\n",
      "Processed 35700/66530 images\n",
      "Processed 35800/66530 images\n",
      "Processed 35900/66530 images\n",
      "Processed 36000/66530 images\n",
      "Processed 36100/66530 images\n",
      "Processed 36200/66530 images\n",
      "Processed 36300/66530 images\n",
      "Processed 36400/66530 images\n",
      "Processed 36500/66530 images\n",
      "Processed 36600/66530 images\n",
      "Processed 36700/66530 images\n",
      "Processed 36800/66530 images\n",
      "Processed 36900/66530 images\n",
      "Processed 37000/66530 images\n",
      "Processed 37100/66530 images\n",
      "Processed 37200/66530 images\n",
      "Processed 37300/66530 images\n",
      "Processed 37400/66530 images\n",
      "Processed 37500/66530 images\n",
      "Processed 37600/66530 images\n",
      "Processed 37700/66530 images\n",
      "Processed 37800/66530 images\n",
      "Processed 37900/66530 images\n",
      "Processed 38000/66530 images\n",
      "Processed 38100/66530 images\n",
      "Processed 38200/66530 images\n",
      "Processed 38300/66530 images\n",
      "Processed 38400/66530 images\n",
      "Processed 38500/66530 images\n",
      "Processed 38600/66530 images\n",
      "Processed 38700/66530 images\n",
      "Processed 38800/66530 images\n",
      "Processed 38900/66530 images\n",
      "Processed 39000/66530 images\n",
      "Processed 39100/66530 images\n",
      "Processed 39200/66530 images\n",
      "Processed 39300/66530 images\n",
      "Processed 39400/66530 images\n",
      "Processed 39500/66530 images\n",
      "Processed 39600/66530 images\n",
      "Processed 39700/66530 images\n",
      "Processed 39800/66530 images\n",
      "Processed 39900/66530 images\n",
      "Processed 40000/66530 images\n",
      "Processed 40100/66530 images\n",
      "Processed 40200/66530 images\n",
      "Processed 40300/66530 images\n",
      "Processed 40400/66530 images\n",
      "Processed 40500/66530 images\n",
      "Processed 40600/66530 images\n",
      "Processed 40700/66530 images\n",
      "Processed 40800/66530 images\n",
      "Processed 40900/66530 images\n",
      "Processed 41000/66530 images\n",
      "Processed 41100/66530 images\n",
      "Processed 41200/66530 images\n",
      "Processed 41300/66530 images\n",
      "Processed 41400/66530 images\n",
      "Processed 41500/66530 images\n",
      "Processed 41600/66530 images\n",
      "Processed 41700/66530 images\n",
      "Processed 41800/66530 images\n",
      "Processed 41900/66530 images\n",
      "Processed 42000/66530 images\n",
      "Processed 42100/66530 images\n",
      "Processed 42200/66530 images\n",
      "Processed 42300/66530 images\n",
      "Processed 42400/66530 images\n",
      "Processed 42500/66530 images\n",
      "Processed 42600/66530 images\n",
      "Processed 42700/66530 images\n",
      "Processed 42800/66530 images\n",
      "Processed 42900/66530 images\n",
      "Processed 43000/66530 images\n",
      "Processed 43100/66530 images\n",
      "Processed 43200/66530 images\n",
      "Processed 43300/66530 images\n",
      "Processed 43400/66530 images\n",
      "Processed 43500/66530 images\n",
      "Processed 43600/66530 images\n",
      "Processed 43700/66530 images\n",
      "Processed 43800/66530 images\n",
      "Processed 43900/66530 images\n",
      "Processed 44000/66530 images\n",
      "Processed 44100/66530 images\n",
      "Processed 44200/66530 images\n",
      "Processed 44300/66530 images\n",
      "Processed 44400/66530 images\n",
      "Processed 44500/66530 images\n",
      "Processed 44600/66530 images\n",
      "Processed 44700/66530 images\n",
      "Processed 44800/66530 images\n",
      "Processed 44900/66530 images\n",
      "Processed 45000/66530 images\n",
      "Processed 45100/66530 images\n",
      "Processed 45200/66530 images\n",
      "Processed 45300/66530 images\n",
      "Processed 45400/66530 images\n",
      "Processed 45500/66530 images\n",
      "Processed 45600/66530 images\n",
      "Processed 45700/66530 images\n",
      "Processed 45800/66530 images\n",
      "Processed 45900/66530 images\n",
      "Processed 46000/66530 images\n",
      "Processed 46100/66530 images\n",
      "Processed 46200/66530 images\n",
      "Processed 46300/66530 images\n",
      "Processed 46400/66530 images\n",
      "Processed 46500/66530 images\n",
      "Processed 46600/66530 images\n",
      "Processed 46700/66530 images\n",
      "Processed 46800/66530 images\n",
      "Processed 46900/66530 images\n",
      "Processed 47000/66530 images\n",
      "Processed 47100/66530 images\n",
      "Processed 47200/66530 images\n",
      "Processed 47300/66530 images\n",
      "Processed 47400/66530 images\n",
      "Processed 47500/66530 images\n",
      "Processed 47600/66530 images\n",
      "Processed 47700/66530 images\n",
      "Processed 47800/66530 images\n",
      "Processed 47900/66530 images\n",
      "Processed 48000/66530 images\n",
      "Processed 48100/66530 images\n",
      "Processed 48200/66530 images\n",
      "Processed 48300/66530 images\n",
      "Processed 48400/66530 images\n",
      "Processed 48500/66530 images\n",
      "Processed 48600/66530 images\n",
      "Processed 48700/66530 images\n",
      "Processed 48800/66530 images\n",
      "Processed 48900/66530 images\n",
      "Processed 49000/66530 images\n",
      "Processed 49100/66530 images\n",
      "Processed 49200/66530 images\n",
      "Processed 49300/66530 images\n",
      "Processed 49400/66530 images\n",
      "Processed 49500/66530 images\n",
      "Processed 49600/66530 images\n",
      "Processed 49700/66530 images\n",
      "Processed 49800/66530 images\n",
      "Processed 49900/66530 images\n",
      "Processed 50000/66530 images\n",
      "Processed 50100/66530 images\n",
      "Processed 50200/66530 images\n",
      "Processed 50300/66530 images\n",
      "Processed 50400/66530 images\n",
      "Processed 50500/66530 images\n",
      "Processed 50600/66530 images\n",
      "Processed 50700/66530 images\n",
      "Processed 50800/66530 images\n",
      "Processed 50900/66530 images\n",
      "Processed 51000/66530 images\n",
      "Processed 51100/66530 images\n",
      "Processed 51200/66530 images\n",
      "Processed 51300/66530 images\n",
      "Processed 51400/66530 images\n",
      "Processed 51500/66530 images\n",
      "Processed 51600/66530 images\n",
      "Processed 51700/66530 images\n",
      "Processed 51800/66530 images\n",
      "Processed 51900/66530 images\n",
      "Processed 52000/66530 images\n",
      "Processed 52100/66530 images\n",
      "Processed 52200/66530 images\n",
      "Processed 52300/66530 images\n",
      "Processed 52400/66530 images\n",
      "Processed 52500/66530 images\n",
      "Processed 52600/66530 images\n",
      "Processed 52700/66530 images\n",
      "Processed 52800/66530 images\n",
      "Processed 52900/66530 images\n",
      "Processed 53000/66530 images\n",
      "Processed 53100/66530 images\n",
      "Processed 53200/66530 images\n",
      "Processed 53300/66530 images\n",
      "Processed 53400/66530 images\n",
      "Processed 53500/66530 images\n",
      "Processed 53600/66530 images\n",
      "Processed 53700/66530 images\n",
      "Processed 53800/66530 images\n",
      "Processed 53900/66530 images\n",
      "Processed 54000/66530 images\n",
      "Processed 54100/66530 images\n",
      "Processed 54200/66530 images\n",
      "Processed 54300/66530 images\n",
      "Processed 54400/66530 images\n",
      "Processed 54500/66530 images\n",
      "Processed 54600/66530 images\n",
      "Processed 54700/66530 images\n",
      "Processed 54800/66530 images\n",
      "Processed 54900/66530 images\n",
      "Processed 55000/66530 images\n",
      "Processed 55100/66530 images\n",
      "Processed 55200/66530 images\n",
      "Processed 55300/66530 images\n",
      "Processed 55400/66530 images\n",
      "Processed 55500/66530 images\n",
      "Processed 55600/66530 images\n",
      "Processed 55700/66530 images\n",
      "Processed 55800/66530 images\n",
      "Processed 55900/66530 images\n",
      "Processed 56000/66530 images\n",
      "Processed 56100/66530 images\n",
      "Processed 56200/66530 images\n",
      "Processed 56300/66530 images\n",
      "Processed 56400/66530 images\n",
      "Processed 56500/66530 images\n",
      "Processed 56600/66530 images\n",
      "Processed 56700/66530 images\n",
      "Processed 56800/66530 images\n",
      "Processed 56900/66530 images\n",
      "Processed 57000/66530 images\n",
      "Processed 57100/66530 images\n",
      "Processed 57200/66530 images\n",
      "Processed 57300/66530 images\n",
      "Processed 57400/66530 images\n",
      "Processed 57500/66530 images\n",
      "Processed 57600/66530 images\n",
      "Processed 57700/66530 images\n",
      "Processed 57800/66530 images\n",
      "Processed 57900/66530 images\n",
      "Processed 58000/66530 images\n",
      "Processed 58100/66530 images\n",
      "Processed 58200/66530 images\n",
      "Processed 58300/66530 images\n",
      "Processed 58400/66530 images\n",
      "Processed 58500/66530 images\n",
      "Processed 58600/66530 images\n",
      "Processed 58700/66530 images\n",
      "Processed 58800/66530 images\n",
      "Processed 58900/66530 images\n",
      "Processed 59000/66530 images\n",
      "Processed 59100/66530 images\n",
      "Processed 59200/66530 images\n",
      "Processed 59300/66530 images\n",
      "Processed 59400/66530 images\n",
      "Processed 59500/66530 images\n",
      "Processed 59600/66530 images\n",
      "Processed 59700/66530 images\n",
      "Processed 59800/66530 images\n",
      "Processed 59900/66530 images\n",
      "Processed 60000/66530 images\n",
      "Processed 60100/66530 images\n",
      "Processed 60200/66530 images\n",
      "Processed 60300/66530 images\n",
      "Processed 60400/66530 images\n",
      "Processed 60500/66530 images\n",
      "Processed 60600/66530 images\n",
      "Processed 60700/66530 images\n",
      "Processed 60800/66530 images\n",
      "Processed 60900/66530 images\n",
      "Processed 61000/66530 images\n",
      "Processed 61100/66530 images\n",
      "Processed 61200/66530 images\n",
      "Processed 61300/66530 images\n",
      "Processed 61400/66530 images\n",
      "Processed 61500/66530 images\n",
      "Processed 61600/66530 images\n",
      "Processed 61700/66530 images\n",
      "Processed 61800/66530 images\n",
      "Processed 61900/66530 images\n",
      "Processed 62000/66530 images\n",
      "Processed 62100/66530 images\n",
      "Processed 62200/66530 images\n",
      "Processed 62300/66530 images\n",
      "Processed 62400/66530 images\n",
      "Processed 62500/66530 images\n",
      "Processed 62600/66530 images\n",
      "Processed 62700/66530 images\n",
      "Processed 62800/66530 images\n",
      "Processed 62900/66530 images\n",
      "Processed 63000/66530 images\n",
      "Processed 63100/66530 images\n",
      "Processed 63200/66530 images\n",
      "Processed 63300/66530 images\n",
      "Processed 63400/66530 images\n",
      "Processed 63500/66530 images\n",
      "Processed 63600/66530 images\n",
      "Processed 63700/66530 images\n",
      "Processed 63800/66530 images\n",
      "Processed 63900/66530 images\n",
      "Processed 64000/66530 images\n",
      "Processed 64100/66530 images\n",
      "Processed 64200/66530 images\n",
      "Processed 64300/66530 images\n",
      "Processed 64400/66530 images\n",
      "Processed 64500/66530 images\n",
      "Processed 64600/66530 images\n",
      "Processed 64700/66530 images\n",
      "Processed 64800/66530 images\n",
      "Processed 64900/66530 images\n",
      "Processed 65000/66530 images\n",
      "Processed 65100/66530 images\n",
      "Processed 65200/66530 images\n",
      "Processed 65300/66530 images\n",
      "Processed 65400/66530 images\n",
      "Processed 65500/66530 images\n",
      "Processed 65600/66530 images\n",
      "Processed 65700/66530 images\n",
      "Processed 65800/66530 images\n",
      "Processed 65900/66530 images\n",
      "Processed 66000/66530 images\n",
      "Processed 66100/66530 images\n",
      "Processed 66200/66530 images\n",
      "Processed 66300/66530 images\n",
      "Processed 66400/66530 images\n",
      "Processed 66500/66530 images\n"
     ]
    }
   ],
   "source": [
    "output_csv = 'outputs/submission_____.csv'\n",
    "\n",
    "_, _, _, start_epoch = load_checkpoint(latest_checkpoint_path(model_dir, \"checkpoint_12.pth\"), model, optimizer)\n",
    "\n",
    "image_files = [f for f in os.listdir(path_to_test_images) if f.endswith('.jpg')]\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image_path = os.path.join(path_to_test_images, image_file)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform_val(image).unsqueeze(0).to(device)\n",
    "        outputs = model(image)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        prob_class_1 = probabilities[0][1].item()  \n",
    "        if (i % 100 == 0):\n",
    "            print(f'Processed {i}/{len(image_files)} images')\n",
    "\n",
    "        results.append((image_file, prob_class_1))\n",
    "\n",
    "df = pd.DataFrame(results, columns=['Name', 'Diagnosis'])\n",
    "df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/checkpoint_11.pth' (epoch 11)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os   \n",
    "\n",
    "from data import OralCancerDataset, RotationTransform\n",
    "from utils import save_checkpoint, load_checkpoint, latest_checkpoint_path\n",
    "from process import train_epoch\n",
    "from models import *\n",
    "batch_size = 224\n",
    "learning_rate = 1e-05\n",
    "betas = (0.9, 0.999)\n",
    "num_workers = 4\n",
    "weight_decay = 3e-05\n",
    "pin_memory = True\n",
    "drop_path_rate = 0.625\n",
    "gamma = 0.7\n",
    "\n",
    "\n",
    "output_csv = 'outputs/submission_all_data_nxt_25_14__.csv'\n",
    "path_to_csv = 'cancer-classification-challenge-2024/train.csv'\n",
    "path_to_train_images = 'cancer-classification-challenge-2024/train'\n",
    "path_to_test_images = 'cancer-classification-challenge-2024/test'\n",
    "model_dir = 'checkpoints/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    RotationTransform(angles=[0, -90, 90, 180]),\n",
    "    transforms.Resize((128, 128)), \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = OralCancerDataset(path_to_csv=path_to_csv, path_to_image=path_to_train_images, transform=transform, full_dataset=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_convnext_tiny_AvgPolling(drop_path_rate=drop_path_rate).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "_, _, _, start_epoch = load_checkpoint(latest_checkpoint_path(model_dir, \"checkpoint_11.pth\"), model, optimizer)\n",
    "\n",
    "train_loss, trian_acc, lr = train_epoch(model, optimizer, None, loss_fn, train_loader, device)\n",
    "save_checkpoint(model, optimizer, None, 0, model_dir, max_checkpoints = 10)\n",
    "    \n",
    "\n",
    "image_files = [f for f in os.listdir(path_to_test_images) if f.endswith('.jpg')]\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image_path = os.path.join(path_to_test_images, image_file)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform_val(image).unsqueeze(0).to(device)\n",
    "        outputs = model(image)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        prob_class_1 = probabilities[0][1].item()  \n",
    "        if (i % 100 == 0):\n",
    "            print(f'Processed {i}/{len(image_files)} images')\n",
    "\n",
    "        results.append((image_file, prob_class_1))\n",
    "\n",
    "df = pd.DataFrame(results, columns=['Name', 'Diagnosis'])\n",
    "df.to_csv(output_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
